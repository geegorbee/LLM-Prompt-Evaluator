# LLM-Prompt-Evaluator
A lightweight Python tool for testing and evaluating LLM responses. Includes prompt engineering samples, manual scoring criteria, and output auditing structure for tone, accuracy, and ethical safety. Built for aspiring AI content reviewers and prompt engineers.
